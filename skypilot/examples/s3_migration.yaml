resources:
  cloud: nebius
  region: eu-north1
  cpus: 16

num_nodes: 8
workdir: .
file_mounts:
  # Assumes AWS profiles for both source and target are configured in
  #  ~/.aws/config and ~/.aws/credentials.
  ~/.aws: ~/.aws
envs:
  # Required variables - must be set before launching
  SOURCE_AWS_PROFILE: default
  SOURCE_ENDPOINT_URL: https://s3.us-east-1.amazonaws.com
  SOURCE_BUCKET:         # Source bucket, e.g. s3://source-bucket     
  TARGET_AWS_PROFILE: nebius
  TARGET_ENDPOINT_URL: https://storage.eu-north1.nebius.cloud:443
  TARGET_BUCKET:            # Target bucket, e.g. s3://target-bucket
  NUM_CONCURRENT: 16 # Number of concurrent transfer processes
  
setup: |
  echo 'Installing required tools...'
  
  # Install s5cmd
  wget https://github.com/peak/s5cmd/releases/download/v2.3.0/s5cmd_2.3.0_Linux-64bit.tar.gz
  tar -xvf s5cmd_2.3.0_Linux-64bit.tar.gz
  sudo mv s5cmd /usr/local/bin/
  rm s5cmd_2.3.0_Linux-64bit.tar.gz
  
  # Install other utilities
  sudo apt-get update
  sudo apt-get install -y jq bc parallel
  
  # Create RAM disk mount point
  sudo mkdir -p /mnt/ramdisk
  # Mount 8GB RAM disk (adjust size as needed based on your VM memory)
  sudo mount -t tmpfs -o size=8g tmpfs /mnt/ramdisk
  # Set permissions for the RAM disk
  sudo chmod 777 /mnt/ramdisk
  # Create temp directory on RAM disk
  mkdir -p /mnt/ramdisk/s3migration/temp
  
run: |
  # Configuration and environment setup
  export TASK_ID=${SKYPILOT_TASK_ID}
  export NODE_RANK=${SKYPILOT_NODE_RANK}
  export NUM_NODES=${SKYPILOT_NUM_NODES}
  export TEMP_DIR=/mnt/ramdisk/s3migration/temp
  export TEMP_S3_PREFIX="_skypilot_temp/${TASK_ID}"
  mkdir -p $TEMP_DIR
  
  # Record start time
  START_TIME=$(date +%s)
  
  echo "Starting S3 migration task: ${TASK_ID}"
  echo "Running on node ${NODE_RANK} of ${NUM_NODES} nodes"
  echo "Migration started at $(date)"
  echo "Using RAM disk for temporary storage at ${TEMP_DIR}"
  
  # Verify that target bucket is accessible
  echo "Verifying target bucket access..."
  if ! s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL ls $TARGET_BUCKET &>/dev/null; then
    echo "Error: Cannot access target bucket. Check credentials and permissions."
    exit 1
  fi
  
  # Head node lists all objects and distributes work
  if [ "${NODE_RANK}" = "0" ]; then
    echo "Head node: Listing all objects in source bucket"
    
    # List all objects
    s5cmd --profile $SOURCE_AWS_PROFILE --endpoint-url $SOURCE_ENDPOINT_URL ls $SOURCE_BUCKET/* > $TEMP_DIR/all_objects.txt
    
    # Filter out directories
    grep -v " DIR " $TEMP_DIR/all_objects.txt > $TEMP_DIR/filtered_objects.txt
    
    # Count total objects and calculate estimated size
    TOTAL_OBJECTS=$(wc -l < $TEMP_DIR/filtered_objects.txt)
    TOTAL_SIZE_BYTES=$(awk '{sum += $3} END {print sum}' $TEMP_DIR/filtered_objects.txt)
    TOTAL_SIZE_GB=$(echo "scale=2; $TOTAL_SIZE_BYTES / 1024 / 1024 / 1024" | bc)
    
    echo "Found $TOTAL_OBJECTS objects with total size of approximately $TOTAL_SIZE_GB GB"
    
    # Distribute objects evenly among nodes
    echo "Distributing objects among $NUM_NODES nodes"
    
    # Split files by node using modulo on line number
    for i in $(seq 0 $((NUM_NODES-1))); do
      awk -v node=$i -v nodes=$NUM_NODES 'NR % nodes == node' $TEMP_DIR/filtered_objects.txt > $TEMP_DIR/node_${i}_objects.txt
      
      # Count files and total size for this node
      NODE_OBJECTS=$(wc -l < $TEMP_DIR/node_${i}_objects.txt)
      NODE_SIZE_BYTES=$(awk '{sum += $3} END {print sum}' $TEMP_DIR/node_${i}_objects.txt)
      NODE_SIZE_GB=$(echo "scale=2; $NODE_SIZE_BYTES / 1024 / 1024 / 1024" | bc)
      
      echo "Node $i assigned $NODE_OBJECTS objects with total size of approximately $NODE_SIZE_GB GB"
      
      # Upload the file list to S3 for each node to access
      s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL \
        cp $TEMP_DIR/node_${i}_objects.txt $TARGET_BUCKET/${TEMP_S3_PREFIX}/node_${i}_objects.txt
    done
    
    echo "Object lists uploaded to S3 for all nodes to access"
  fi
  
  # Wait for head node to finish distribution
  sleep 2
  
  # Each node downloads its assigned object list
  echo "Node ${NODE_RANK}: Downloading assigned object list"
  
  # Try multiple times with exponential backoff
  retry_count=0
  max_retries=3
  success=false
  
  while [ $retry_count -lt $max_retries ] && [ "$success" = false ]; do
    if s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL \
      cp $TARGET_BUCKET/${TEMP_S3_PREFIX}/node_${NODE_RANK}_objects.txt $TEMP_DIR/my_objects.txt; then
      success=true
      echo "Successfully downloaded object list"
    else
      retry_count=$((retry_count+1))
      sleep_time=$((2 ** retry_count))
      echo "Attempt $retry_count failed. Waiting $sleep_time seconds before retry..."
      sleep $sleep_time
    fi
  done
  
  if [ "$success" = false ]; then
    echo "Error: Failed to download object list after $max_retries attempts."
    exit 1
  fi
  
  # Count objects assigned to this node
  MY_OBJECTS=$(wc -l < $TEMP_DIR/my_objects.txt)
  if [ $MY_OBJECTS -eq 0 ]; then
    echo "No objects assigned to node ${NODE_RANK}. Exiting."
    
    # Upload completion marker
    echo "Node ${NODE_RANK} completed with no objects at $(date)" > $TEMP_DIR/node_${NODE_RANK}_done
    s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL \
      cp $TEMP_DIR/node_${NODE_RANK}_done $TARGET_BUCKET/${TEMP_S3_PREFIX}/node_${NODE_RANK}_done
    
    exit 0
  fi
  
  echo "Node ${NODE_RANK}: Processing $MY_OBJECTS objects"
  
  # Create a function to process a single object
  process_object() {
    local line="$1"
    local size=$(echo $line | awk '{print $3}')
    local object_path=$(echo $line | awk '{print $NF}')
    local object_key=${object_path#$SOURCE_BUCKET/}
    local relative_path=${object_key}
    local local_file_path="${TEMP_DIR}/${relative_path}"
    
    # Create directory structure if needed
    mkdir -p "$(dirname "$local_file_path")"
    
    echo "Processing: ${object_key}"
    
    # Download the object
    if s5cmd --profile $SOURCE_AWS_PROFILE --endpoint-url $SOURCE_ENDPOINT_URL \
         cp --concurrency $NUM_CONCURRENT "${SOURCE_BUCKET}/${object_key}" "${local_file_path}"; then
      
      # Upload the object to the target
      if s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL \
           cp --concurrency $NUM_CONCURRENT "${local_file_path}" "${TARGET_BUCKET}/${object_key}"; then
        echo "Successfully migrated: ${object_key}"
      else
        echo "Error: Failed to upload ${object_key}"
      fi
    else
      echo "Error: Failed to download ${object_key}"
    fi
    
    # Remove the temporary file
    rm -f "${local_file_path}"
  }
  
  # Export the function for use with parallel
  export -f process_object
  export SOURCE_AWS_PROFILE SOURCE_ENDPOINT_URL SOURCE_BUCKET
  export TARGET_AWS_PROFILE TARGET_ENDPOINT_URL TARGET_BUCKET
  export TEMP_DIR
  
  # Process objects in parallel using GNU parallel
  cat $TEMP_DIR/my_objects.txt | parallel -j $NUM_CONCURRENT process_object
  
  # Signal completion by uploading a marker to S3
  echo "Node ${NODE_RANK} completed all objects at $(date)" > $TEMP_DIR/node_${NODE_RANK}_done
  s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL \
    cp $TEMP_DIR/node_${NODE_RANK}_done $TARGET_BUCKET/${TEMP_S3_PREFIX}/node_${NODE_RANK}_done
  
  echo "Node ${NODE_RANK}: Migration completed!"
  
  # Head node verifies all nodes have completed
  if [ "${NODE_RANK}" = "0" ]; then
    echo "Head node: Waiting for all worker nodes to complete"
    
    # Wait for all workers to complete
    for worker in $(seq 1 $((NUM_NODES-1))); do
      echo "Waiting for worker $worker to complete..."
      
      while true; do
        if s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL \
          ls $TARGET_BUCKET/${TEMP_S3_PREFIX}/node_${worker}_done &>/dev/null; then
          echo "Worker $worker has completed"
          break
        fi
        sleep 2
      done
    done
    
    echo "All workers have completed. Verifying object counts..."
    
    # Count objects in source and target buckets
    s5cmd --profile $SOURCE_AWS_PROFILE --endpoint-url $SOURCE_ENDPOINT_URL ls $SOURCE_BUCKET/* > $TEMP_DIR/final_source_objects.txt
    s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL ls $TARGET_BUCKET/* > $TEMP_DIR/final_target_objects.txt
    
    # Filter out directory entries and temp files
    grep -v " DIR " $TEMP_DIR/final_source_objects.txt > $TEMP_DIR/final_source_files.txt
    grep -v " DIR " $TEMP_DIR/final_target_objects.txt | grep -v "_skypilot_temp" > $TEMP_DIR/final_target_files.txt
    
    # Extract just the object keys for comparison
    cat $TEMP_DIR/final_source_files.txt | awk '{print $NF}' | sort > $TEMP_DIR/source_keys.txt
    cat $TEMP_DIR/final_target_files.txt | awk '{print $NF}' | sort > $TEMP_DIR/target_keys.txt
    
    # Compare counts
    SOURCE_COUNT=$(wc -l < $TEMP_DIR/final_source_files.txt)
    TARGET_COUNT=$(wc -l < $TEMP_DIR/final_target_files.txt)
    
    # Calculate and print migration duration
    END_TIME=$(date +%s)
    DURATION=$((END_TIME - START_TIME))
    HOURS=$((DURATION / 3600))
    MINUTES=$(( (DURATION % 3600) / 60 ))
    SECONDS=$((DURATION % 60))
    
    echo "------------------------------------------------------"
    echo "📊 Migration Statistics:"
    echo "Total migration time: ${HOURS}h ${MINUTES}m ${SECONDS}s"
    echo "Source bucket: $SOURCE_COUNT objects"
    echo "Target bucket: $TARGET_COUNT objects"
    echo "------------------------------------------------------"
    
    if [ $SOURCE_COUNT -eq $TARGET_COUNT ]; then
      echo "✅ Migration completed successfully! Object counts match."
      
      # Even if counts match, check for differences in file lists
      if diff $TEMP_DIR/source_keys.txt $TEMP_DIR/target_keys.txt > $TEMP_DIR/diff_output.txt; then
        echo "✅ All objects match between source and target buckets."
      else
        echo "⚠️ Warning: Although counts match, some objects differ between buckets."
        echo "Objects in source but not in target (first 10):"
        grep "^<" $TEMP_DIR/diff_output.txt | head -10
        echo "Objects in target but not in source (first 10):"
        grep "^>" $TEMP_DIR/diff_output.txt | head -10
      fi
    else
      echo "⚠️ Migration completed with warnings. Object counts don't match."
      echo "Source: $SOURCE_COUNT, Target: $TARGET_COUNT"
      
      # Show detailed differences
      echo "Analyzing differences between source and target buckets..."
      
      # Find files in source but not in target
      comm -23 $TEMP_DIR/source_keys.txt $TEMP_DIR/target_keys.txt > $TEMP_DIR/missing_in_target.txt
      MISSING_TARGET=$(wc -l < $TEMP_DIR/missing_in_target.txt)

      echo "$MISSING_TARGET files exist in source but not in target"
      
      if [ $MISSING_TARGET -gt 0 ]; then
        echo "Sample of missing files in target (first 10):"
        head -10 $TEMP_DIR/missing_in_target.txt
      fi
      
    fi
    
    # Clean up temporary files in S3
    s5cmd --profile $TARGET_AWS_PROFILE --endpoint-url $TARGET_ENDPOINT_URL \
      rm $TARGET_BUCKET/${TEMP_S3_PREFIX}/*
      
    # Clean up RAM disk temp files to free memory
    echo "Cleaning up RAM disk files..."
    rm -rf $TEMP_DIR/*
  fi