resources:
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      name: terraform-fluxcd-values
    data:
      values.yaml: |
        helmRepository:
          interval: 15m
        certManager:
          enabled: true
          interval: 5m
          timeout: 5m
          %{~ if cert_manager_version != "" ~}
          version: ${cert_manager_version}
          %{~ endif ~}
        backup:
          enabled: ${backups_enabled}
          interval: 5m
          timeout: 5m
          %{~ if k8up_version != "" ~}
          version: ${k8up_version}
          %{~ endif ~}
          values: null
        mariadbOperator:
          enabled: ${accounting_enabled}
          interval: 5m
          timeout: 5m
          %{~ if mariadb_operator_version != "" ~}
          version: ${mariadb_operator_version}
          %{~ endif ~}
          values: null
        observability:
          enabled: ${telemetry_enabled}
          opentelemetry:
            enabled: ${telemetry_enabled}
            clusterName: ${cluster_name}
            publicEndpointEnabled: ${public_o11y_enabled}
            logs:
              %{~ if opentelemetry_collector_version != "" ~}
              version: ${opentelemetry_collector_version}
              %{~ endif ~}
              interval: 5m
              timeout: 5m
              values:
                publicEndpointEnabled: ${public_o11y_enabled}
                resources:
                  requests:
                    memory: ${resources.logs_collector.memory}
                    cpu: ${resources.logs_collector.cpu}
                  limits:
                    memory: ${resources.logs_collector.memory}
            events:
              %{~ if opentelemetry_collector_version != "" ~}
              version: ${opentelemetry_collector_version}
              %{~ endif ~}
              interval: 5m
              timeout: 5m
              values:
                publicEndpointEnabled: ${public_o11y_enabled}
                resources:
                  requests:
                    memory: ${resources.events_collector.memory}
                    cpu: ${resources.events_collector.cpu}
                  limits:
                    memory: ${resources.events_collector.memory}
          prometheusOperator:
            enabled: ${telemetry_enabled}
            interval: 5m
            timeout: 5m
            %{~ if prometheus_crds_version != "" ~}
            version: ${prometheus_crds_version}
            %{~ endif ~}
          vmStack:
            enabled: ${telemetry_enabled}
            crds:
              interval: 5m
              timeout: 5m
              %{~ if vmstack_crds_version != "" ~}
              version: ${vmstack_crds_version}
              %{~ endif ~}
            interval: 5m
            timeout: 5m
            %{~ if vmstack_version != "" ~}
            version: ${vmstack_version}
            %{~ endif ~}
            values:
              vmagent:
                spec:
                  extraArgs:
                    promscrape.maxScrapeSize: "33554432"
                  resources:
                    requests:
                      memory: ${resources.vm_agent.memory}
                      cpu: ${resources.vm_agent.cpu}
                    limits:
                      memory: ${resources.vm_agent.memory}
                  %{~ if public_o11y_enabled ~}
                  remoteWrite:
                    - bearerTokenSecret:
                        key: accessToken
                        name: o11y-writer-sa-token
                      url: https://write.monitoring.eu-north1.nebius.cloud./projects/project-e00h61cxzwnf6zksvdn77/prometheus/api/v1/write
                      inlineUrlRelabelConfig:
                      - regex: "feature_node_kubernetes_io_.*|kubernetes_io_.*|nvidia_com_gpu.*"
                        action: labeldrop
                      - regex: "UUID|DCGM_FI_PROCESS_NAME|pci_bus_id|prometheus|DCGM_FI_DEV_SERIAL|DCGM_FI_DEV_NAME|job"
                        action: labeldrop
                    - url: http://vmsingle-metrics-victoria-metrics-k8s-stack.monitoring-system.svc.cluster.local.:8429/api/v1/write
                  remoteWriteSettings:
                    label:
                      cluster: ${cluster_name}
                    queues: 2
                  %{~ endif ~}
              vmsingle:
                spec:
                  extraArgs:
                    dedup.minScrapeInterval: 30s
                    maxLabelsPerTimeseries: "40"
                    search.maxQueryLen: "18765"
                    search.maxUniqueTimeseries: "500000"
                  retentionPeriod: 30d
                  extraEnvs:
                  - name: GOMAXPROCS
                    value: "${resources.vm_single.gomaxprocs}"
                  %{~ if create_pvcs ~}
                  storage:
                    accessModes:
                      - ReadWriteOnce
                    resources:
                      requests:
                        storage: ${resources.vm_single.size}
                  %{~ endif ~}
                  resources:
                    requests:
                      memory: ${resources.vm_single.memory}
                      cpu: ${resources.vm_single.cpu}
                    limits:
                      memory: ${resources.vm_single.memory}
          vmLogs:
            enabled: ${telemetry_enabled}
            interval: 5m
            timeout: 5m
            %{~ if vmlogs_version != "" ~}
            version: ${vmlogs_version}
            %{~ endif ~}
            values:
              persistentVolume:
              %{~ if create_pvcs ~}
                enabled: true
                size: ${resources.vm_logs.size}
                accessMode: ReadWriteOnce
              %{~ else ~}
                enabled: false
              %{~ endif ~}
              resources:
                requests:
                  memory: ${resources.vm_logs.memory}
                  cpu: ${resources.vm_logs.cpu}
                limits:
                  memory: ${resources.vm_logs.memory}
          dcgmExporter:
            enabled: ${dcgm_job_mapping_enabled}
            version: ${operator_version}
            values:
              hpcJobMapDir: ${dcgm_job_map_dir}
              resources:
                requests:
                  memory: ${resources.dcgm_exporter.memory}Gi
                  cpu: ${resources.dcgm_exporter.cpu}m
                limits:
                  memory: ${resources.dcgm_exporter.memory}Gi

        securityProfilesOperator:
          enabled: ${apparmor_enabled}
          interval: 5m
          timeout: 5m
          %{~ if security_profiles_operator_version != "" ~}
          version: ${security_profiles_operator_version}
          %{~ endif ~}
          values: null
        slurmCluster:
          enabled: true
          interval: 5m
          timeout: 5m
          version: ${operator_version}
          namespace: soperator
          releaseName: soperator
          overrideValues: 
            clusterName: ${name}
            clusterType: ${slurm_cluster.nodes.worker.resources.gpus > 0 ? "gpu" : "cpu" }
            useDefaultAppArmorProfile: ${apparmor_enabled}
            maintenance: ${slurm_cluster.maintenance}

            partitionConfiguration:
              configType: ${slurm_cluster.partition_configuration.slurm_config_type}
              %{~ if slurm_cluster.partition_configuration.slurm_config_type == "custom" ~}
              rawConfig:
                %{~ for partition in slurm_cluster.partition_configuration.slurm_raw_config ~}
                - "${partition}"
                %{~ endfor ~}
              %{~ endif ~}

            workerFeatures:
              %{~ for feature in slurm_cluster.slurm_worker_features ~}
              - name: ${feature.name}
                hostlistExpr: "${feature.hostlist_expr}"
                nodesetName: "${feature.nodeset_name}"
              %{~ endfor ~}

            %{~ if slurm_cluster.slurm_health_check_config != null ~}
            healthCheckConfig:
              healthCheckInterval: ${slurm_cluster.slurm_health_check_config.health_check_interval}
              healthCheckProgram: ${slurm_cluster.slurm_health_check_config.health_check_program}
              healthCheckNodeState:
              %{~ for state in slurm_cluster.slurm_health_check_config.health_check_node_state ~}
                - state: ${state.state}
              %{~ endfor ~}
            %{~ endif ~}

            k8sNodeFilters:
              - name: ${slurm_cluster.k8s_node_filters.system.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                                - ${slurm_cluster.k8s_node_filters.system.match}

              - name: ${slurm_cluster.k8s_node_filters.accounting.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                                - ${slurm_cluster.k8s_node_filters.accounting.match}
                tolerations:
                  - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                    operator: Equal
                    value: ${slurm_cluster.k8s_node_filters.label.accounting}
                    effect: NoSchedule

              - name: ${slurm_cluster.k8s_node_filters.controller.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                                - ${slurm_cluster.k8s_node_filters.controller.match}
                tolerations:
                  - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                    operator: Equal
                    value: ${slurm_cluster.k8s_node_filters.label.controller}
                    effect: NoSchedule

              - name: ${slurm_cluster.k8s_node_filters.login.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                                - ${slurm_cluster.k8s_node_filters.login.match}
                tolerations:
                  - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                    operator: Equal
                    value: ${slurm_cluster.k8s_node_filters.label.login}
                    effect: NoSchedule

              - name: ${slurm_cluster.k8s_node_filters.worker.name}
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                        - matchExpressions:
                            - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                              operator: In
                              values:
                              %{~ for worker in slurm_cluster.k8s_node_filters.worker.matches ~}
                                - "${worker}"
                              %{~ endfor ~}
                tolerations:
                  - key: ${slurm_cluster.k8s_node_filters.label.nodeset}
                    operator: Equal
                    value: ${slurm_cluster.k8s_node_filters.label.worker}
                    effect: NoSchedule
                %{~ if slurm_cluster.nodes.worker.resources.gpus > 0 ~}
                  - key: ${slurm_cluster.k8s_node_filters.label.gpu}
                    operator: Exists
                    effect: NoSchedule
                %{~ endif ~}

            volumeSources:
              - name: jail
                persistentVolumeClaim:
                  claimName: jail-pvc
                  readOnly: false

              - name: controller-spool
                persistentVolumeClaim:
                  claimName: controller-spool-pvc
                  readOnly: false

              - name: worker-spool
                emptyDir:
                  sizeLimit: ${slurm_cluster.nodes.worker.resources.ephemeral_storage}Gi

              %{~ if slurm_cluster.nfs.enabled ~}
              - name: nfs
                nfs:
                  path: ${slurm_cluster.nfs.path}
                  readOnly: false
                  server: ${slurm_cluster.nfs.host}
              %{~ endif ~}

              %{~ if telemetry_enabled ~}
              - name: motd-nebius-o11y
                configMap:
                  name: motd-nebius-o11y
                  defaultMode: 500
              %{~ endif ~}

              %{~ for sub_mount in slurm_cluster.jail_submounts ~}
              - name: jail-submount-${sub_mount.name}
                persistentVolumeClaim:
                  claimName: jail-submount-${sub_mount.name}-pvc
                  readOnly: false
              %{~ endfor ~}

              - name: sys-host
                hostPath:
                  path: /sys
                  type: Directory

              %{~ if slurm_cluster.default_prolog_enabled ~}
              - name: slurm-prolog
                configMap:
                  name: slurm-prolog
                  defaultMode: 500
              %{~ endif ~}

              %{~ if slurm_cluster.default_epilog_enabled ~}
              - name: slurm-epilog
                configMap:
                  name: slurm-epilog
                  defaultMode: 500
              %{~ endif ~}

              %{~ if slurm_cluster.node_local_image_storage.enabled ~}
              - name: image-storage
                configMap:
                  name: image-storage
                  defaultMode: 500
              %{~ endif ~}

              %{~ if dcgm_job_mapping_enabled ~}
              - name: host-var
                hostPath:
                  path: /var
                  type: ""
              - name: hpc-jobs-dir
                hostPath:
                  path: ${dcgm_job_map_dir}
                  type: Directory
              %{~ endif ~}

            populateJail:
              k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.system.name}

            ncclSettings:
              topologyType: "${slurm_cluster.nccl_topology_type}"

            periodicChecks:
              ncclBenchmark:
                enabled: ${slurm_cluster.nccl_benchmark.enable}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.controller.name}
                schedule: "${slurm_cluster.nccl_benchmark.schedule}"
                ncclArguments:
                  thresholdMoreThan: ${slurm_cluster.nccl_benchmark.min_threshold}
                  useInfiniband: ${slurm_cluster.nccl_benchmark.use_infiniband}

            slurmConfig:
              defMemPerNode: ${floor(slurm_cluster.nodes.worker.resources.memory * 1024)}
              %{~ if slurm_cluster.default_prolog_enabled ~}
              prolog: /etc/slurm-scripts/prolog.sh
              %{~ endif ~}
              %{~ if slurm_cluster.default_epilog_enabled ~}
              epilog: /etc/slurm-scripts/epilog.sh
              %{~ endif ~}

            slurmNodes:
              accounting:
                enabled: ${accounting_enabled}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.accounting.name}
                %{~ if slurm_cluster.nodes.accounting.enabled ~}
                mariadbOperator:
                  enabled: ${slurm_cluster.nodes.accounting.mariadb_operator.enabled}
                  %{~ if slurm_cluster.nodes.accounting.mariadb_operator.enabled  ~}
                  protectedSecret: ${slurm_cluster.nodes.accounting.use_protected_secret}
                  resources:
                    cpu: ${slurm_cluster.nodes.accounting.mariadb_operator.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.accounting.mariadb_operator.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.accounting.mariadb_operator.resources.ephemeral_storage}Gi
                  metrics:
                    enabled: ${slurm_cluster.nodes.accounting.mariadb_operator.metrics_enabled}
                  storage:
                    ephemeral: false
                    volumeClaimTemplate:
                      accessModes:
                        - ReadWriteOnce
                      resources:
                        requests:
                          storage: ${slurm_cluster.nodes.accounting.mariadb_operator.storage_size}Gi
                  %{~ endif ~}
                %{~ if length(slurm_cluster.nodes.accounting.slurmdbd_config) > 0 ~}
                slurmdbdConfig:
                  %{~ for key, value in slurm_cluster.nodes.accounting.slurmdbd_config ~}
                  ${slurm_cluster.key}: %{ if value == "yes" || value == "no" }"${slurm_cluster.value}"%{ else }${slurm_cluster.value}%{ endif }
                  %{~ endfor ~}
                %{~ endif ~}
                %{~ if length(slurm_cluster.nodes.accounting.slurm_config) > 0 ~}
                slurmConfig:
                  %{~ for key, value in slurm_cluster.nodes.accounting.slurm_config ~}
                  ${slurm_cluster.key}: %{ if value == "yes" || value == "no" }"${slurm_cluster.value}"%{ else }${slurm_cluster.value}%{ endif }
                  %{~ endfor ~}
                %{~ endif ~}
                slurmdbd:
                  resources:
                    cpu: ${slurm_cluster.nodes.accounting.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.accounting.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.accounting.resources.ephemeral_storage}Gi
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                %{~ endif ~}

              controller:
                size: ${slurm_cluster.nodes.controller.size}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.controller.name}
                slurmctld:
                  resources:
                    cpu: ${slurm_cluster.nodes.controller.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.controller.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.controller.resources.ephemeral_storage}Gi
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]

              worker:
                size: ${slurm_cluster.nodes.worker.size}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.worker.name}
                cgroupVersion: v2
                enableGDRCopy: true
                slurmNodeExtra: "${slurm_cluster.nodes.worker.slurm_node_extra}"
                sshdConfigMapRefName: "${slurm_cluster.nodes.worker.sshd_config_map_ref_name}"
                supervisordConfigMapRefName: custom-supervisord-config
                slurmd:
                  resources:
                    cpu: ${slurm_cluster.nodes.worker.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.worker.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.worker.resources.ephemeral_storage}Gi
                    gpu: ${slurm_cluster.nodes.worker.resources.gpus}
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                  %{~ for sub_mount in slurm_cluster.jail_submounts ~}
                  - name: ensure-jail-submount-${sub_mount.name}-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: '${sub_mount.name}', mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                  %{~ endfor ~}
                  %{~ for sub_mount in slurm_cluster.node_local_jail_submounts ~}
                  - name: ensure-node-local-jail-submount-${sub_mount.name}-${sub_mount.filesystem_type}
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: '${sub_mount.name}', mountPath: /volume-mount }]
                    command: ["grep", ' /volume-mount ${sub_mount.filesystem_type} ', "/proc/mounts"]
                  %{~ endfor ~}
                  %{~ if slurm_cluster.node_local_image_storage.enabled ~}
                  - name: ensure-node-local-image-storage-${slurm_cluster.node_local_image_storage.spec.filesystem_type}
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: 'image-storage', mountPath: /volume-mount }]
                    command: ["grep", ' /volume-mount ${slurm_cluster.node_local_image_storage.spec.filesystem_type} ', "/proc/mounts"]
                  - name: prepare-node-local-image-storage
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: 'image-storage', mountPath: /volume-mount }]
                    command:
                    - "sh"
                    - "-c"
                    - |
                      mkdir -p \
                      %{~ for dir in ["cache", "data", "runtime"] ~}
                        /volume-mount/enroot/${dir} \
                      %{~ endfor ~}
                        /volume-mount/docker \
                      && chmod 777 -R /volume-mount/enroot /volume-mount/docker
                  %{~ endif ~}
                  %{~ if dcgm_job_mapping_enabled ~}
                  - name: create-hpc-jobs-dir
                    command: ["sh", "-c", ' mkdir -p ${dcgm_job_map_dir} ']
                    image: cr.eu-north1.nebius.cloud/soperator/busybox:latest
                    securityContext:
                      privileged: true
                    volumeMounts:
                    - mountPath: /var
                      mountPropagation: HostToContainer
                      name: host-var
                  %{~ endif ~}
                volumes:
                  spool:
                    volumeClaimTemplateSpec: null
                    volumeSourceName: worker-spool
                  jailSubMounts:
                    %{~ if slurm_cluster.nfs.enabled ~}
                    - mountPath: ${slurm_cluster.nfs.mount_path}
                      name: nfs
                      volumeSourceName: nfs
                    %{~ endif ~}
                    %{~ for sub_mount in slurm_cluster.jail_submounts ~}
                    - name: ${sub_mount.name}
                      mountPath: ${sub_mount.mount_path}
                      volumeSourceName: jail-submount-${sub_mount.name}
                    %{~ endfor ~}
                    %{~ for sub_mount in slurm_cluster.node_local_jail_submounts ~}
                    - name: ${sub_mount.name}
                      mountPath: ${sub_mount.mount_path}
                      volumeClaimTemplateSpec:
                        accessModes:
                          - ReadWriteOnce
                        storageClassName: ${sub_mount.storage_class_name}
                        resources:
                          requests:
                            storage: ${sub_mount.size_gibibytes}Gi
                    %{~ endfor ~}
                    %{~ if slurm_cluster.node_local_image_storage.enabled ~}
                    - name: image-storage
                      mountPath: /mnt/image-storage
                      volumeClaimTemplateSpec:
                        accessModes:
                          - ReadWriteOnce
                        storageClassName: ${slurm_cluster.node_local_image_storage.spec.storage_class_name}
                        resources:
                          requests:
                            storage: ${slurm_cluster.node_local_image_storage.spec.size_gibibytes}Gi
                    %{~ endif ~}
                  customMounts:
                    - name: sys-host
                      mountPath: /mnt/jail.upper/sys-host
                      readOnly: true
                      volumeSourceName: sys-host
                    %{~ if slurm_cluster.default_prolog_enabled ~}
                    - name: slurm-prolog
                      mountPath: /etc/slurm-scripts/prolog.sh
                      subPath: prolog.sh
                      readOnly: true
                      volumeSourceName: slurm-prolog
                    %{~ endif ~}
                    %{~ if slurm_cluster.default_epilog_enabled ~}
                    - name: slurm-epilog
                      mountPath: /etc/slurm-scripts/epilog.sh
                      subPath: epilog.sh
                      readOnly: true
                      volumeSourceName: slurm-epilog
                    %{~ endif ~}
                    %{~ if slurm_cluster.node_local_image_storage.enabled ~}
                    - name: enroot-on-image-storage
                      mountPath: /etc/enroot/enroot.conf.d/image-storage.conf
                      subPath: enroot.conf
                      readOnly: true
                      volumeSourceName: image-storage
                    - name: docker-on-image-storage
                      mountPath: /etc/docker/daemon.json
                      subPath: daemon.json
                      readOnly: true
                      volumeSourceName: image-storage
                    %{~ endif ~}
                    %{~ if dcgm_job_mapping_enabled ~}
                    - name: hpc-jobs-dir
                      mountPath: ${dcgm_job_map_dir}
                    %{~ endif ~}
                  sharedMemorySize: ${slurm_cluster.nodes.worker.shared_memory}Gi

              login:
                size: ${slurm_cluster.nodes.login.size}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.login.name}
                sshdServiceType: "LoadBalancer"
                %{~ if slurm_cluster.nodes.login.allocation_id != null ~}
                sshdServiceAnnotations:
                  "nebius.com/load-balancer-allocation-id": "${slurm_cluster.nodes.login.allocation_id}"
                %{~ endif ~}
                %{~ if length(slurm_cluster.nodes.login.root_public_keys) > 0 ~}
                sshdConfigMapRefName: "${slurm_cluster.nodes.login.sshd_config_map_ref_name}"
                sshRootPublicKeys:
                  %{~ for key in slurm_cluster.nodes.login.root_public_keys ~}
                  - ${key}
                  %{~ endfor ~}
                %{~ endif ~}
                sshd:
                  resources:
                    cpu: ${slurm_cluster.nodes.login.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.login.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.login.resources.ephemeral_storage}Gi
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                  %{~ for sub_mount in slurm_cluster.jail_submounts ~}
                  - name: ensure-jail-submount-${sub_mount.name}-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: '${sub_mount.name}', mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                  %{~ endfor ~}
                volumes:
                  jailSubMounts:
                    %{~ if slurm_cluster.nfs.enabled ~}
                    - mountPath: ${slurm_cluster.nfs.mount_path}
                      name: nfs
                      volumeSourceName: nfs
                    %{~ endif ~}
                    %{~ if telemetry_enabled ~}
                    - mountPath: /etc/update-motd.d/95-nebius-o11y
                      subPath: 95-nebius-o11y
                      readOnly: true
                      name: motd-nebius-o11y
                      volumeSourceName: motd-nebius-o11y
                    %{~ endif ~}
                    %{~ for sub_mount in slurm_cluster.jail_submounts ~}
                    - name: ${sub_mount.name}
                      mountPath: ${sub_mount.mount_path}
                      volumeSourceName: jail-submount-${sub_mount.name}
                    %{~ endfor ~}
                  customMounts:
                    - name: sys-host
                      mountPath: /mnt/jail.upper/sys-host
                      readOnly: true
                      volumeSourceName: sys-host
                    %{~ if slurm_cluster.default_prolog_enabled ~}
                    - name: slurm-prolog
                      mountPath: /etc/slurm-scripts/prolog.sh
                      subPath: prolog.sh
                      readOnly: true
                      volumeSourceName: slurm-prolog
                    %{~ endif ~}
                    %{~ if slurm_cluster.default_epilog_enabled ~}
                    - name: slurm-epilog
                      mountPath: /etc/slurm-scripts/epilog.sh
                      subPath: epilog.sh
                      readOnly: true
                      volumeSourceName: slurm-epilog
                    %{~ endif ~}

              exporter:
                enabled: ${slurm_cluster.nodes.exporter.enabled}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.system.name}
                exporter:
                  resources:
                    cpu: ${slurm_cluster.nodes.exporter.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.exporter.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.exporter.resources.ephemeral_storage}Gi
                munge:
                  resources:
                    cpu: ${slurm_cluster.nodes.munge.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.munge.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.munge.resources.ephemeral_storage}Gi
                customInitContainers:
                  - name: ensure-jail-virtiofs
                    image: cr.eu-north1.nebius.cloud/soperator/busybox
                    volumeMounts: [{ name: jail, mountPath: /volume-mount }]
                    command: ["grep", " /volume-mount virtiofs ", "/proc/mounts"]
                volumes:
                  jail:
                    volumeSourceName: "jail"

              rest:
                enabled: ${slurm_cluster.nodes.rest.enabled}
                k8sNodeFilterName: ${slurm_cluster.k8s_node_filters.system.name}
                rest:
                  resources:
                    cpu: ${slurm_cluster.nodes.rest.resources.cpu * 1000}m
                    memory: ${slurm_cluster.nodes.rest.resources.memory}Gi
                    ephemeralStorage: ${slurm_cluster.nodes.rest.resources.ephemeral_storage}Gi

            sConfigController:
              node:
                k8sNodeFilterName: ${slurm_cluster.sconfigcontroller.node.k8s_node_filter_name }
                size: ${slurm_cluster.sconfigcontroller.node.size }
              container:
                imagePullPolicy: ${slurm_cluster.sconfigcontroller.container.image_pull_policy }
                resources:
                  cpu: ${slurm_cluster.sconfigcontroller.container.resources.cpu }m
                  memory: ${slurm_cluster.sconfigcontroller.container.resources.memory }Mi
                  ephemeralStorage: ${slurm_cluster.sconfigcontroller.container.resources.ephemeral_storage }Mi
              
          slurmClusterStorage:
            enabled: true
            interval: 5m
            timeout: 5m
            overrideValues:
              volume:
                controllerSpool:
                  size: ${slurm_cluster_storage.volume.controller_spool.size}
                  filestoreDeviceName: ${slurm_cluster_storage.volume.controller_spool.device}
                jail:
                  size: ${slurm_cluster_storage.volume.jail.size}
                  filestoreDeviceName: ${slurm_cluster_storage.volume.jail.device}
                %{~ if length(slurm_cluster_storage.volume.jail_submounts) > 0 ~}
                jailSubMounts:
                  %{~ for sub_mount in slurm_cluster_storage.volume.jail_submounts ~}
                  - name: ${sub_mount.name}
                    size: ${sub_mount.size}
                    filestoreDeviceName: ${sub_mount.device}
                  %{~ endfor ~}
                %{~ endif ~}
                accounting:
                  enabled: ${slurm_cluster_storage.volume.accounting.enabled}
                  %{~ if slurm_cluster_storage.volume.accounting.enabled ~}
                  size: ${slurm_cluster_storage.volume.accounting.size}
                  filestoreDeviceName: ${slurm_cluster_storage.volume.accounting.device}
                  %{~ endif ~}

              storage:
                accounting:
                  matchExpressions:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: In
                      values:
                        - ${slurm_cluster_storage.scheduling.accounting.match}
                  tolerations:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: Equal
                      value: ${slurm_cluster_storage.scheduling.label.accounting}
                      effect: NoSchedule

                controllerSpool:
                  matchExpressions:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: In
                      values:
                        - ${slurm_cluster_storage.scheduling.controller.match}
                  tolerations:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: Equal
                      value: ${slurm_cluster_storage.scheduling.label.controller}
                      effect: NoSchedule

                jail:
                  matchExpressions:
                    - key: slurm.nebius.ai/jail
                      operator: In
                      values:
                        - "true"
                  tolerations:
                    - key: ${slurm_cluster_storage.scheduling.label.nodeset}
                      operator: Exists
                      effect: NoSchedule
                  %{~ if slurm_cluster_storage.scheduling.worker.gpu_present ~}
                    - key: ${slurm_cluster_storage.scheduling.label.gpu}
                      operator: Exists
                      effect: NoSchedule
                  %{~ endif ~}

        soperator:
          enabled: true
          interval: 5m
          timeout: 5m
          version: ${operator_version}
          values:
            manager:
              resources: {}
          kruise:
            enabled: true
            interval: 5m
            timeout: 5m
          soperatorChecks:
            enabled: true
            interval: 5m
            timeout: 5m
            values:
              resources:
                requests:
                  memory: ${resources.slurm_checks.requests.memory}Gi
                  cpu: ${resources.slurm_checks.requests.cpu * 1000}m
                limits:
                  memory: ${resources.slurm_checks.limits.memory}Gi
          nodeConfigurator:
            enabled: true
            interval: 5m
            timeout: 5m
            values:
              resources:
                requests:
                  memory: ${resources.node_configurator.requests.memory}Gi
                  cpu: ${resources.node_configurator.requests.cpu * 1000}m
                limits:
                  memory: ${resources.node_configurator.limits.memory}Gi
