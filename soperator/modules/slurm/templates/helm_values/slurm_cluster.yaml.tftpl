clusterName: ${name}
clusterType: ${ nodes.worker.resources.gpus > 0 ? "gpu" : "cpu" }
useDefaultAppArmorProfile: ${useDefaultAppArmorProfile}
maintenance: ${maintenance}

partitionConfiguration:
  configType: ${partition_configuration.slurm_config_type}
  %{~ if partition_configuration.slurm_config_type == "custom" ~}
  rawConfig:
    %{~ for partition in partition_configuration.slurm_raw_config ~}
    - "${partition}"
    %{~ endfor ~}
  %{~ endif ~}

k8sNodeFilters:
  - name: ${k8s_node_filters.system.name}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: ${k8s_node_filters.label.nodeset}
                  operator: In
                  values:
                    - ${k8s_node_filters.system.match}

  - name: ${k8s_node_filters.accounting.name}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: ${k8s_node_filters.label.nodeset}
                  operator: In
                  values:
                    - ${k8s_node_filters.accounting.match}
    tolerations:
      - key: ${k8s_node_filters.label.nodeset}
        operator: Equal
        value: ${k8s_node_filters.label.accounting}
        effect: NoSchedule

  - name: ${k8s_node_filters.controller.name}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: ${k8s_node_filters.label.nodeset}
                  operator: In
                  values:
                    - ${k8s_node_filters.controller.match}
    tolerations:
      - key: ${k8s_node_filters.label.nodeset}
        operator: Equal
        value: ${k8s_node_filters.label.controller}
        effect: NoSchedule

  - name: ${k8s_node_filters.login.name}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: ${k8s_node_filters.label.nodeset}
                  operator: In
                  values:
                    - ${k8s_node_filters.login.match}
    tolerations:
      - key: ${k8s_node_filters.label.nodeset}
        operator: Equal
        value: ${k8s_node_filters.label.login}
        effect: NoSchedule

  - name: ${k8s_node_filters.worker.name}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: ${k8s_node_filters.label.nodeset}
                  operator: In
                  values:
                  %{~ for worker in k8s_node_filters.worker.matches ~}
                    - "${worker}"
                  %{~ endfor ~}
    tolerations:
      - key: ${k8s_node_filters.label.nodeset}
        operator: Equal
        value: ${k8s_node_filters.label.worker}
        effect: NoSchedule
    %{~ if nodes.worker.resources.gpus > 0 ~}
      - key: ${k8s_node_filters.label.gpu}
        operator: Exists
        effect: NoSchedule
    %{~ endif ~}

volumeSources:
  - name: jail
    persistentVolumeClaim:
      claimName: jail-pvc
      readOnly: false

  - name: controller-spool
    persistentVolumeClaim:
      claimName: controller-spool-pvc
      readOnly: false

  - name: worker-spool
    emptyDir:
      sizeLimit: ${nodes.worker.resources.ephemeral_storage}Gi

  %{~ if nfs.enabled ~}
  - name: nfs
    nfs:
      path: ${nfs.path}
      readOnly: false
      server: ${nfs.host}
  %{~ endif ~}

  %{~ if telemetry.enabled ~}
  - name: motd-nebius-o11y
    configMap:
      name: motd-nebius-o11y
      defaultMode: 500
  %{~ endif ~}

  %{~ for sub_mount in jail_submounts ~}
  - name: jail-submount-${sub_mount.name}
    persistentVolumeClaim:
      claimName: jail-submount-${sub_mount.name}-pvc
      readOnly: false
  %{~ endfor ~}

  - name: sys-host
    hostPath:
      path: /sys
      type: Directory

  %{~ if default_prolog_enabled ~}
  - name: slurm-prolog
    configMap:
      name: slurm-prolog
      defaultMode: 500
  %{~ endif ~}

  %{~ if default_epilog_enabled ~}
  - name: slurm-epilog
    configMap:
      name: slurm-epilog
      defaultMode: 500
  %{~ endif ~}

populateJail:
  k8sNodeFilterName: ${k8s_node_filters.system.name}

ncclSettings:
  topologyType: "${nccl_topology_type}"

periodicChecks:
  ncclBenchmark:
    enabled: ${nccl_benchmark.enable}
    k8sNodeFilterName: ${k8s_node_filters.controller.name}
    schedule: "${nccl_benchmark.schedule}"
    ncclArguments:
      thresholdMoreThan: ${nccl_benchmark.min_threshold}
      useInfiniband: ${nccl_benchmark.use_infiniband}

slurmConfig:
  defMemPerNode: ${floor(nodes.worker.resources.memory * 1024)}
  %{~ if default_prolog_enabled ~}
  prolog: /etc/slurm-scripts/prolog.sh
  %{~ endif ~}
  %{~ if default_epilog_enabled ~}
  epilog: /etc/slurm-scripts/epilog.sh
  %{~ endif ~}

slurmNodes:
  accounting:
    enabled: ${nodes.accounting.enabled}
    k8sNodeFilterName: ${k8s_node_filters.accounting.name}
    %{~ if nodes.accounting.enabled ~}
    mariadbOperator:
      enabled: ${nodes.accounting.mariadb_operator.enabled}
      %{~ if nodes.accounting.mariadb_operator.enabled  ~}
      protectedSecret: ${nodes.accounting.use_protected_secret}
      resources:
        cpu: ${nodes.accounting.mariadb_operator.resources.cpu * 1000}m
        memory: ${nodes.accounting.mariadb_operator.resources.memory}Gi
        ephemeralStorage: ${nodes.accounting.mariadb_operator.resources.ephemeral_storage}Gi
      metrics:
        enabled: ${nodes.accounting.mariadb_operator.metrics_enabled}
      storage:
        ephemeral: false
        volumeClaimTemplate:
          accessModes:
            - ReadWriteMany
          resources:
            requests:
              storage: ${nodes.accounting.mariadb_operator.storage_size}Gi
          volumeName: accounting-pv
          storageClassName: slurm-local-pv
        storageClassName: slurm-local-pv
      %{~ endif ~}
    %{~ if length(nodes.accounting.slurmdbd_config) > 0 ~}
    slurmdbdConfig:
      %{~ for key, value in nodes.accounting.slurmdbd_config ~}
      ${key}: %{ if value == "yes" || value == "no" }"${value}"%{ else }${value}%{ endif }
      %{~ endfor ~}
    %{~ endif ~}
    %{~ if length(nodes.accounting.slurm_config) > 0 ~}
    slurmConfig:
      %{~ for key, value in nodes.accounting.slurm_config ~}
      ${key}: %{ if value == "yes" || value == "no" }"${value}"%{ else }${value}%{ endif }
      %{~ endfor ~}
    %{~ endif ~}
    slurmdbd:
      resources:
        cpu: ${nodes.accounting.resources.cpu * 1000}m
        memory: ${nodes.accounting.resources.memory}Gi
        ephemeralStorage: ${nodes.accounting.resources.ephemeral_storage}Gi
    munge:
      resources:
        cpu: ${nodes.munge.resources.cpu * 1000}m
        memory: ${nodes.munge.resources.memory}Gi
        ephemeralStorage: ${nodes.munge.resources.ephemeral_storage}Gi
    %{~ endif ~}

  controller:
    size: ${nodes.controller.size}
    k8sNodeFilterName: ${k8s_node_filters.controller.name}
    slurmctld:
      resources:
        cpu: ${nodes.controller.resources.cpu * 1000}m
        memory: ${nodes.controller.resources.memory}Gi
        ephemeralStorage: ${nodes.controller.resources.ephemeral_storage}Gi
    munge:
      resources:
        cpu: ${nodes.munge.resources.cpu * 1000}m
        memory: ${nodes.munge.resources.memory}Gi
        ephemeralStorage: ${nodes.munge.resources.ephemeral_storage}Gi

  worker:
    size: ${nodes.worker.size}
    k8sNodeFilterName: ${k8s_node_filters.worker.name}
    cgroupVersion: v2
    enableGDRCopy: true
    slurmNodeExtra: "${nodes.worker.slurm_node_extra}"
    sshdConfigMapRefName: "${nodes.worker.sshd_config_map_ref_name}"
    supervisordConfigMapRefName: custom-supervisord-config
    slurmd:
      resources:
        cpu: ${nodes.worker.resources.cpu * 1000}m
        memory: ${nodes.worker.resources.memory}Gi
        ephemeralStorage: ${nodes.worker.resources.ephemeral_storage}Gi
        gpu: ${nodes.worker.resources.gpus}
    munge:
      resources:
        cpu: ${nodes.munge.resources.cpu * 1000}m
        memory: ${nodes.munge.resources.memory}Gi
        ephemeralStorage: ${nodes.munge.resources.ephemeral_storage}Gi
    volumes:
      spool:
        volumeClaimTemplateSpec: null
        volumeSourceName: worker-spool
      jailSubMounts:
        %{~ if nfs.enabled ~}
        - mountPath: ${nfs.mount_path}
          name: nfs
          volumeSourceName: nfs
        %{~ endif ~}
        %{~ for sub_mount in jail_submounts ~}
        - name: ${sub_mount.name}
          mountPath: ${sub_mount.mount_path}
          volumeSourceName: jail-submount-${sub_mount.name}
        %{~ endfor ~}
      customMounts:
        - name: sys-host
          mountPath: /mnt/jail.upper/sys-host
          readOnly: true
          volumeSourceName: sys-host
        %{~ if default_prolog_enabled ~}
        - name: slurm-prolog
          mountPath: /etc/slurm-scripts/prolog.sh
          subPath: prolog.sh
          readOnly: true
          volumeSourceName: slurm-prolog
        %{~ endif ~}
        %{~ if default_epilog_enabled ~}
        - name: slurm-epilog
          mountPath: /etc/slurm-scripts/epilog.sh
          subPath: epilog.sh
          readOnly: true
          volumeSourceName: slurm-epilog
        %{~ endif ~}
      sharedMemorySize: ${nodes.worker.shared_memory}Gi

  login:
    size: ${nodes.login.size}
    k8sNodeFilterName: ${k8s_node_filters.login.name}
    sshdServiceType: "LoadBalancer"
    %{~ if nodes.login.allocation_id != null ~}
    sshdServiceAnnotations:
      "nebius.com/load-balancer-allocation-id": "${nodes.login.allocation_id}"
    %{~ endif ~}
    %{~ if length(nodes.login.root_public_keys) > 0 ~}
    sshdConfigMapRefName: "${nodes.login.sshd_config_map_ref_name}"
    sshRootPublicKeys:
      %{~ for key in nodes.login.root_public_keys ~}
      - ${key}
      %{~ endfor ~}
    %{~ endif ~}
    sshd:
      resources:
        cpu: ${nodes.login.resources.cpu * 1000}m
        memory: ${nodes.login.resources.memory}Gi
        ephemeralStorage: ${nodes.login.resources.ephemeral_storage}Gi
    munge:
      resources:
        cpu: ${nodes.munge.resources.cpu * 1000}m
        memory: ${nodes.munge.resources.memory}Gi
        ephemeralStorage: ${nodes.munge.resources.ephemeral_storage}Gi
    volumes:
      jailSubMounts:
        %{~ if nfs.enabled ~}
        - mountPath: ${nfs.mount_path}
          name: nfs
          volumeSourceName: nfs
        %{~ endif ~}
        %{~ if telemetry.enabled ~}
        - mountPath: /etc/update-motd.d/95-nebius-o11y
          subPath: 95-nebius-o11y
          readOnly: true
          name: motd-nebius-o11y
          volumeSourceName: motd-nebius-o11y
        %{~ endif ~}
        %{~ for sub_mount in jail_submounts ~}
        - name: ${sub_mount.name}
          mountPath: ${sub_mount.mount_path}
          volumeSourceName: jail-submount-${sub_mount.name}
        %{~ endfor ~}
      customMounts:
        - name: sys-host
          mountPath: /mnt/jail.upper/sys-host
          readOnly: true
          volumeSourceName: sys-host
        %{~ if default_prolog_enabled ~}
        - name: slurm-prolog
          mountPath: /etc/slurm-scripts/prolog.sh
          subPath: prolog.sh
          readOnly: true
          volumeSourceName: slurm-prolog
        %{~ endif ~}
        %{~ if default_epilog_enabled ~}
        - name: slurm-epilog
          mountPath: /etc/slurm-scripts/epilog.sh
          subPath: epilog.sh
          readOnly: true
          volumeSourceName: slurm-epilog
        %{~ endif ~}

  exporter:
    enabled: ${nodes.exporter.enabled}
    k8sNodeFilterName: ${k8s_node_filters.system.name}
    exporter:
      resources:
        cpu: ${nodes.exporter.resources.cpu * 1000}m
        memory: ${nodes.exporter.resources.memory}Gi
        ephemeralStorage: ${nodes.exporter.resources.ephemeral_storage}Gi
    munge:
      resources:
        cpu: ${nodes.munge.resources.cpu * 1000}m
        memory: ${nodes.munge.resources.memory}Gi
        ephemeralStorage: ${nodes.munge.resources.ephemeral_storage}Gi
    volumes:
      jail:
        volumeSourceName: "jail"

  rest:
    enabled: ${nodes.rest.enabled}
    k8sNodeFilterName: ${k8s_node_filters.system.name}
    rest:
      resources:
        cpu: ${nodes.rest.resources.cpu * 1000}m
        memory: ${nodes.rest.resources.memory}Gi
        ephemeralStorage: ${nodes.rest.resources.ephemeral_storage}Gi

%{ if telemetry.enabled ~}
telemetry:
  jobsTelemetry:
    otelCollectorHttpHost: ${telemetry.metrics_collector.endpoint.http_host}
    otelCollectorPort: ${telemetry.metrics_collector.endpoint.port}
    otelCollectorPath: /opentelemetry/api/v1/push
    sendJobsEvents: true
    sendOtelMetrics: true
%{ endif }
