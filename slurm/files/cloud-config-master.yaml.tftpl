#cloud-config
hostname: ${hostname}
prefer_fqdn_over_hostname: false
users:
  - name: slurm
    groups: sudo
    shell: /bin/bash
    sudo: 'ALL=(ALL) NOPASSWD:ALL'
    ssh-authorized-keys:
      - ${chomp(ssh_public_key)}
      - ${chomp(master_public_key)}

packages:
  - unzip
  - python3-pip
  %{~ if shared_fs_type == "nfs" ~}
  - nfs-common
  %{~ endif ~}

runcmd:
  - mkdir -p /home/slurm
  - chmod 700 /home/slurm
  - cp /home/ubuntu/.bashrc /home/slurm/
  - cp /etc/ssh/id_rsa /home/slurm/.ssh/
  - chown -R slurm:slurm /home/slurm
  %{~ if shared_fs_type == "filesystem" ~}
  - mkdir /mnt/slurm
  - echo "slurm-fs /mnt/slurm virtiofs rw 0 0" >> /etc/fstab
  - mount -a
  %{~ endif ~}
  %{~ if shared_fs_type == "nfs" ~}
  - mkdir /mnt/nfs
  - echo "${nfs_ip}:${nfs_export_path} /mnt/nfs nfs defaults 0 0" >> /etc/fstab
  - mount -a
  %{~ endif ~}
  - base64 -d /tmp/ansible/ansible_role.zip.b64 > /tmp/ansible/ansible_role.zip
  - unzip /tmp/ansible/ansible_role.zip -d /tmp/ansible/
  - pip3 install ansible
  - |
    ANSIBLE_HOST_KEY_CHECKING=False \
    ANSIBLE_LOG_PATH=/tmp/ansible/ansible.log \
    ansible-playbook -i /tmp/ansible/inventory.yml /tmp/ansible/slurm.yml & echo $! > /tmp/ansible/ansible.pid
# export ANSIBLE_HOST_KEY_CHECKING=False
# ansible-playbook -i /tmp/ansible/inventory.yml -e SLURM_BINARIES="12.2.2-jammy-slurm" -e SLURM_VERSION="24.05.3" -e ENROOT_VERSION="3.4.1" /tmp/ansible/slurm.yml

write_files:
  - content: ${ansible_role}
    encoding: gzip+base64
    path: /tmp/ansible/ansible_role.zip.b64   
  - content: |
      all:
        vars:
          SLURM_VERSION: ${SLURM_VERSION}
          SLURM_BINARIES: ${SLURM_BINARIES}
          ENROOT_VERSION: ${ENROOT_VERSION}
          PMIX_VERSION: ${PMIX_VERSION}
      slurm_master:
        hosts:
          slurm-master:
      slurm_worker:
        hosts:
          %{~ for name, ip in slurm_workers_ip ~}
          ${name}:
            ansible_host: ${ip}
          %{~ endfor ~}
    path: /tmp/ansible/inventory.yml
  - content: |
      # slurm.conf file
      ClusterName=slurm-cluster
      SlurmctldHost=slurm-master
      SlurmUser=slurm
      SlurmdUser=root
      SlurmctldPort=6817
      SlurmdPort=6818
      AuthType=auth/munge
      StateSaveLocation=/var/lib/slurm/slurmctld
      SwitchType=switch/none
      MpiDefault=pmi2
      SlurmctldPidFile=/run/slurmctld.pid
      SlurmdPidFile=/run/slurmd.pid
      ProctrackType=proctrack/pgid
      ReturnToService=0
      PlugStackConfig=/etc/slurm/plugstack.conf
      SlurmctldParameters=enable_configless
      # TIMERS
      SlurmctldTimeout=300
      SlurmdTimeout=300
      InactiveLimit=0
      MinJobAge=300
      KillWait=30
      Waittime=0
      # DEBUG
      DebugFlags=NO_CONF_HASH
      # LOGGING/ACCOUNTNG
      SlurmctldDebug=info
      SlurmctldLogFile=/var/log/slurm/slurmctld.log
      SlurmdDebug=info
      SlurmdLogFile=/var/log/slurm/slurmd.log
      JobAcctGatherType=jobacct_gather/none
      #DB
      %{~ if is_mysql ~}
      AccountingStorageType=accounting_storage/slurmdbd
      AccountingStorageHost=slurm-master
      JobCompType=jobcomp/mysql
      JobCompUser=slurm
      JobCompPass=${password}
      JobCompHost=${hostname}
      JobCompLoc=slurm-db
      %{~ endif ~}
      GresTypes=gpu
      SelectType=select/cons_tres
      # COMPUTE NODES
      # NodeName=${worker_prefix}-[1-${cluster_workers_count}] CPUs=16 RealMemory=32090 State=idle State=UNKNOWN
      NodeName=${worker_prefix}-[1-${cluster_workers_count}] Gres=gpu:8 CPUs=128 RealMemory=1290080 State=idle State=UNKNOWN
      PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP
    path: /etc/slurm/slurm.conf
  - content: |
      AuthType=auth/munge
      DbdHost=slurm-master
      DebugLevel=info
      LogFile=/var/log/slurm/slurmdbd.log
      PidFile=/run/slurmdbd.pid
      SlurmUser=slurm
      StoragePass=${password}
      StorageUser=slurm
      StorageHost=${hostname}
      StorageLoc=slurm-db
      StorageType=accounting_storage/mysql
    path: /etc/slurm/slurmdbd.conf
    permissions: '0600'
  - content: |
      cluster:
        all: '@slurm'
        clients: 'client[01-${cluster_workers_count}]'
    path: /etc/clustershell/groups.d/cluster.yaml
  - content: |
      ${chomp(indent(6,master_private_key))}
    path: /etc/ssh/id_rsa
    permissions: "0400"
    owner: "root:root"
